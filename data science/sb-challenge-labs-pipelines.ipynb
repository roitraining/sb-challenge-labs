{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 ROI Training, Inc. \n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic"
   },
   "source": [
    "# Vertex AI Pipelines: Automating your AutoML Tabular workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:pipelines,automl,beans",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:pipelines,automl",
    "tags": []
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this activity you will learn to use `Vertex AI Pipelines` and `Google Cloud Pipeline Components` to build the same `AutoML`  classification model you created in the AutoML activity\n",
    "\n",
    "### Main Tasks\n",
    "\n",
    "- Create a pipeline that will:\n",
    "    - Create a dataset\n",
    "    - Train an AutoML classification model\n",
    "    - Create an endpoint\n",
    "    - Deploys your model\n",
    "- Compile the pipeline\n",
    "- Execute the KFP pipeline using **Vertex AI Pipelines**\n",
    "\n",
    "Documentation for the Google Cloud Pipelines components can be found [ here](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.4.1/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:pipelines,automl,beans"
   },
   "source": [
    "## 1. Prepare your notebook instance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "install_aip:mbsdk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required Python packages\n",
    "\n",
    "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
    "                                 google-cloud-storage \\\n",
    "                                 kfp \\\n",
    "                                 google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "set_project_id",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760420608781-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# Set required Python and environment variables\n",
    "\n",
    "import random\n",
    "import string\n",
    "import sys\n",
    "import uuid\n",
    "\n",
    "PROJECT_ID = \"sb-challenge-labs\" #[your-project-id]\"\n",
    "REGION = \"us-central1\"\n",
    "UUID = uuid.uuid4().hex\n",
    "\n",
    "shell_output = !gcloud auth list 2>/dev/null\n",
    "SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "print(SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "# Configure Google Cloud SDK on Workbench instance\n",
    "\n",
    "! gcloud config set project {PROJECT_ID}\n",
    "! gcloud config set compute/region {REGION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:pipelines,automl,beans",
    "tags": []
   },
   "source": [
    "## 2. Prepare your project\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MzGDU7TWdts_",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://sb-challenge-labs-pipelines/...\n"
     ]
    }
   ],
   "source": [
    "# create a bucket in which to store intermediate artifacts\n",
    "\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}-pipelines\"\n",
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "set_service_account:pipelines",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# give your workbench instance's service account required permissions\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:pipelines,automl,beans"
   },
   "source": [
    "## 3. Define the pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "import_aip:mbsdk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import libraries for pipelines\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import (Artifact, ClassificationMetrics, Input, Metrics, Output, component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aip_constants:endpoint",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set path for storing the pipeline artifacts\n",
    "\n",
    "pipeline_name = f\"sb-challenge-labs-pipelines-training-{UUID}\"\n",
    "pipeline_root = f\"{BUCKET_URI}/pipeline_root/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "define_pipeline:gcpc,beans,lcn"
   },
   "outputs": [],
   "source": [
    "# define the pipeline\n",
    "\n",
    "import textwrap\n",
    "\n",
    "@kfp.dsl.pipeline(name=pipeline_name, pipeline_root=pipeline_root)\n",
    "def pipeline(\n",
    "    gcs_source: str,\n",
    "    dataset_name: str,\n",
    "    job_name: str,\n",
    "    model_name: str,\n",
    "    endpoint_name: str,\n",
    "    machine_type: str,\n",
    "    project: str,\n",
    "    gcp_region: str,\n",
    "    thresholds_dict_str: str,\n",
    "    predict_name: str,\n",
    "):\n",
    "    from google_cloud_pipeline_components.v1.bigquery import BigqueryQueryJobOp\n",
    "    from google_cloud_pipeline_components.v1.automl.training_job import AutoMLTabularTrainingJobRunOp\n",
    "    from google_cloud_pipeline_components.v1.dataset.create_tabular_dataset.component import tabular_dataset_create as TabularDatasetCreateOp\n",
    "    from google_cloud_pipeline_components.v1.endpoint.create_endpoint.component import endpoint_create as EndpointCreateOp\n",
    "    from google_cloud_pipeline_components.v1.endpoint ModelDeployOp\n",
    "    from google_cloud_pipeline_components.v1.batch_predict_job import ModelBatchPredictOp\n",
    "    \n",
    "    bigquery_query_job_op = BigqueryQueryJobOp(\n",
    "        project=project,\n",
    "        location=\"US\",\n",
    "        query = textwrap.dedent(f\"\"\"\n",
    "            create or replace view `{project}.ml_data.income-census-training-no-fw` as\n",
    "            select\n",
    "                * except(functional_weight)\n",
    "            from\n",
    "                `bigquery-public-data.ml_datasets.census_adult_income`\n",
    "        \"\"\")\n",
    "    )\n",
    "    \n",
    "    dataset_create_op = TabularDatasetCreateOp(\n",
    "        project=project,\n",
    "        location=gcp_region,\n",
    "        display_name=dataset_name,\n",
    "        bq_source=f\"bq://{project}.ml_data.income-census-training-no-fw\",\n",
    "    ).after(bigquery_query_job_op)\n",
    "\n",
    "    training_op = AutoMLTabularTrainingJobRunOp(\n",
    "        project=project,\n",
    "        location=gcp_region,\n",
    "        display_name=job_name,\n",
    "        optimization_prediction_type=\"classification\",\n",
    "        budget_milli_node_hours=1000,\n",
    "        model_display_name=model_name,\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        target_column=\"income_bracket\"\n",
    "    )\n",
    "\n",
    "    endpoint_op = EndpointCreateOp(\n",
    "        project=project,\n",
    "        location=gcp_region,\n",
    "        display_name=endpoint_name,\n",
    "    )\n",
    "\n",
    "    deploy_op = ModelDeployOp(\n",
    "        model=training_op.outputs[\"model\"],\n",
    "        endpoint=endpoint_op.outputs[\"endpoint\"],\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "        dedicated_resources_machine_type=machine_type,\n",
    "    )\n",
    "    \n",
    "#     predict_op = ModelBatchPredictOp(\n",
    "#         project=project,\n",
    "#         job_display_name=predict_name,\n",
    "#         location=gcp_region,\n",
    "#         bigquery_output_table=f\"`{project}.ml_data.kp_predictions`\",\n",
    "        \n",
    "        \n",
    "#     )\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline"
   },
   "source": [
    "## 4. Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "compile_pipeline"
   },
   "outputs": [],
   "source": [
    "# compile the pipeline\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"tabular_classification_pipeline.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline"
   },
   "source": [
    "## 4. Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "# initialize the Vertex AI SDK connection\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "1adf9b056954"
   },
   "outputs": [],
   "source": [
    "# Set variables used to launch the pipeline\n",
    "\n",
    "pipeline_name = f\"pipeline_{UUID}\"\n",
    "dataset_name = f\"dataset_{UUID}\"\n",
    "model_name = f\"model_{UUID}\"\n",
    "job_name = f\"job_{UUID}\"\n",
    "endpoint_name = f\"endpoint_{UUID}\"\n",
    "machine_type = \"n1-standard-4\"\n",
    "gcs_source = \"gs://sb-challenge-labs/adult-income.csv\"\n",
    "predict_name = f\"predict_{UUID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "run_pipeline:model",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validate region of the given source (BigQuery) against region of the pipeline\n",
    "\n",
    "# Configure the pipeline\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=pipeline_name,\n",
    "    template_path=\"tabular_classification_pipeline.yaml\",\n",
    "    pipeline_root=pipeline_root,\n",
    "    parameter_values={\n",
    "        \"project\": PROJECT_ID,\n",
    "        \"gcp_region\": REGION,\n",
    "        \"gcs_source\": gcs_source,\n",
    "        \"thresholds_dict_str\": '{\"auRoc\": 0.95}',\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"job_name\": job_name,\n",
    "        \"model_name\": model_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"predict_name\": predict_name,\n",
    "        \"machine_type\": machine_type,\n",
    "    },\n",
    "    enable_caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view_pipeline_run:model"
   },
   "source": [
    "Run the pipeline job. Click on the generated link to see your run in the Cloud Console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "114ab8ff24ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223?project=760420608781\n",
      "PipelineJob projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/760420608781/locations/us-central1/pipelineJobs/pipeline-32ad5fad41cd487fbc4496b7f629d79c-20231007183223 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "# Run the job\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:pipelines"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "(Set `delete_bucket` to **True** to delete the Cloud Storage bucket.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:pipelines"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "delete_bucket = False\n",
    "\n",
    "# Delete the Vertex AI Pipeline Job\n",
    "job.delete()\n",
    "\n",
    "# Delete the Vertex AI Endpoint\n",
    "endpoints = aiplatform.Endpoint.list(\n",
    "    filter=f\"display_name={ENDPOINT_DISPLAY_NAME}\", order_by=\"create_time\"\n",
    ")\n",
    "\n",
    "if len(endpoints) > 0:\n",
    "    endpoint = endpoints[0]\n",
    "    endpoint.delete(force=True)\n",
    "\n",
    "# Delete the Vertex AI model\n",
    "models = aiplatform.Model.list(\n",
    "    filter=f\"display_name={MODEL_DISPLAY_NAME}\", order_by=\"create_time\"\n",
    ")\n",
    "if len(models) > 0:\n",
    "    model = models[0]\n",
    "    model.delete()\n",
    "\n",
    "# Delete the Vertex AI Dataset\n",
    "datasets = aiplatform.TabularDataset.list(\n",
    "    filter=f\"display_name={DATASET_DISPLAY_NAME}\", order_by=\"create_time\"\n",
    ")\n",
    "if len(datasets) > 0:\n",
    "    dataset = datasets[0]\n",
    "    dataset.delete()\n",
    "\n",
    "# Delete the Cloud Storage bucket\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "automl_tabular_classification_beans.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
